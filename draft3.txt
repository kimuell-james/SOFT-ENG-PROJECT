# Directory: YourProject/
# ├── data/
# │   └── synthetic_student_data.csv
# ├── pages/
# │   ├── 01_Home.py
# │   ├── 02_Evaluation_Metrics.py
# │   ├── 03_Student_Insights.py
# │   └── 04_Data_Overview.py
# ├── dataloader.py
# ├── evaluation.py
# ├── feature_selection.py
# ├── logistic_regression.py
# ├── visualizations.py

# --- dataloader.py ---
import pandas as pd

def load_data(path):
    return pd.read_csv(path)

# --- feature_selection.py ---
from sklearn.feature_selection import chi2, mutual_info_classif, SelectKBest
import pandas as pd
import numpy as np

class FeatureSelector:
    def __init__(self, df, target_column):
        self.df = df
        self.target_column = target_column
        self.selected_features_per_grade = {}

    def select_features(self, grade):
        grade_features = [col for col in self.df.columns if col.startswith(f'g{grade}_')]
        X = self.df[grade_features]
        y = self.df[self.target_column]

        chi2_selector = SelectKBest(score_func=chi2, k='all')
        chi2_selector.fit(X, y)
        chi2_scores = pd.Series(chi2_selector.scores_, index=X.columns)

        mi_scores = pd.Series(mutual_info_classif(X, y), index=X.columns)

        top = list(set(
            chi2_scores[chi2_scores > np.percentile(chi2_scores, 50)].index
        ).intersection(
            mi_scores[mi_scores > np.percentile(mi_scores, 50)].index
        ))

        self.selected_features_per_grade[grade] = top
        return top

# --- logistic_regression.py ---
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

class LogisticRegressionModel:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.predictions = {}

    def train(self, df, selected_features, target_column, grade):
        X = df[selected_features]
        y = df[target_column]

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        max_iter = max(1000, df.shape[0] // 2)
        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=max_iter)
        model.fit(X_train_scaled, y_train)

        y_pred = model.predict(X_test_scaled)
        y_prob = model.predict_proba(X_test_scaled)

        self.models[grade] = model
        self.scalers[grade] = scaler
        self.predictions[grade] = {
            'y_test': y_test,
            'y_pred': y_pred,
            'y_prob': y_prob
        }
        return model, y_test, y_pred, y_prob

# --- evaluation.py ---
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

class EvaluationMetrics:
    def __init__(self, predictions):
        self.predictions = predictions

    def report(self, grade):
        y_test = self.predictions[grade]['y_test']
        y_pred = self.predictions[grade]['y_pred']
        return classification_report(y_test, y_pred)

    def confusion(self, grade):
        y_test = self.predictions[grade]['y_test']
        y_pred = self.predictions[grade]['y_pred']
        return confusion_matrix(y_test, y_pred)

    def roc_auc(self, grade):
        y_test = self.predictions[grade]['y_test']
        y_prob = self.predictions[grade]['y_prob']
        return roc_auc_score(pd.get_dummies(y_test), y_prob)

# --- visualizations.py ---
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

class Visualizations:
    @staticmethod
    def track_distribution(df, target_column):
        sns.countplot(x=target_column, data=df)
        plt.title("Track Rate Distribution")
        plt.show()

    @staticmethod
    def correlation_matrix(df):
        plt.figure(figsize=(12, 8))
        sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
        plt.title("Correlation Matrix")
        plt.show()

    @staticmethod
    def roc_curve_plot(y_test, y_prob):
        from sklearn.preprocessing import label_binarize
        from sklearn.metrics import roc_curve, auc
        import numpy as np

        y_test_bin = label_binarize(y_test, classes=list(set(y_test)))
        fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())
        roc_auc = auc(fpr, tpr)

        plt.figure()
        plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
        plt.plot([0, 1], [0, 1], linestyle='--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.show()

    @staticmethod
    def feature_importance_plot(model, features):
        coef = pd.Series(model.coef_[0], index=features)
        coef = coef.sort_values()
        coef.plot(kind='barh', title="Feature Importance (Logistic Regression Coefficients)")
        plt.xlabel("Coefficient Value")
        plt.tight_layout()
        plt.show()

# # -----------------------------------------------
# # Correlation Matrix
# st.subheader("Correlation Matrix")

# corr_matrix = df.corr(numeric_only=True)

# plt.figure(figsize=(12, 8))
# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True)
# st.pyplot(plt)

# # Unstack and filter pairs with high correlation (excluding self-correlations)
# high_corr_pairs = (
#     corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
#     .stack()
#     .reset_index()
# )

# high_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']
# high_corr_pairs = high_corr_pairs[
#     (high_corr_pairs['Correlation'] >= 0.8) | (high_corr_pairs['Correlation'] <= -0.8)
# ]

# st.subheader("Highly Correlated Features (|r| ≥ 0.8)")
# st.write(high_corr_pairs)