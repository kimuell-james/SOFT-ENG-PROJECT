import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import chi2, mutual_info_classif, SelectKBest
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

class FeatureSelection:
    def __init__(self, df):
        self.df = df
        self.top_features = []
    
    def select_features(self):
        X = self.df.drop(columns=["SHS_Track"])
        y = self.df["SHS_Track"]
        
        chi2_selector = SelectKBest(score_func=chi2, k='all')
        chi2_selector.fit(X, y)
        chi2_scores = pd.Series(chi2_selector.scores_, index=X.columns).sort_values(ascending=False)
        
        mi_scores = mutual_info_classif(X, y)
        mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)
        
        self.top_features = list(set(chi2_scores[chi2_scores > np.percentile(chi2_scores, 50)].index) &
                                 set(mi_scores[mi_scores > np.percentile(mi_scores, 50)].index))
        return self.top_features

class LogisticRegressionModel:
    def __init__(self, df, top_features):
        self.df = df
        self.top_features = top_features
        self.model = None
        self.scaler = StandardScaler()
    
    def train_model(self):
        X = self.df[self.top_features]
        y = self.df["SHS_Track"]
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        self.model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)
        self.model.fit(X_train_scaled, y_train)
        
        y_pred = self.model.predict(X_test_scaled)
        
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        print("\nConfusion Matrix:")
        print(confusion_matrix(y_test, y_pred))
        
    def get_model(self):
        return self.model

class FeatureImportance:
    def __init__(self, model, top_features):
        self.model = model
        self.top_features = top_features
    
    def calculate_importance(self):
        if self.model is None:
            raise ValueError("Model has not been trained yet.")
        
        importance = pd.Series(self.model.coef_[0], index=self.top_features).sort_values(ascending=False)
        print("Feature Importance (Logistic Regression Coefficients):")
        print(importance)
        return importance

class Visualization:
    def __init__(self, df, model, top_features):
        self.df = df
        self.model = model
        self.top_features = top_features
    
    def plot_average_grades(self):
        df_correct_predictions = self.df[self.df["SHS_Track"] == self.model.predict(self.df[self.top_features])]
        avg_grades = df_correct_predictions.groupby("SHS_Track")[self.top_features].mean()
        
        plt.figure(figsize=(10, 6))
        sns.heatmap(avg_grades, annot=True, cmap="coolwarm", fmt=".2f")
        plt.title("Average Grades per Track")
        plt.xlabel("Subjects")
        plt.ylabel("SHS Track")
        plt.show()

# Usage
file_path = "your_dataset.csv"  # Replace with actual file path
df = pd.read_csv(file_path)

# Feature Selection
feature_selector = FeatureSelection(df)
top_features = feature_selector.select_features()

# Train Logistic Regression Model
model_trainer = LogisticRegressionModel(df, top_features)
model_trainer.train_model()
model = model_trainer.get_model()

# Feature Importance
feature_importance = FeatureImportance(model, top_features)
feature_importance.calculate_importance()

# Visualization
visualization = Visualization(df, model, top_features)
visualization.plot_average_grades()

